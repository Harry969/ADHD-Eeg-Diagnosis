{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We retrieve the total matrix of standardized data: `total_list_stand`.\n",
    "- We apply KNN with GridSearch and Cross-Validation (CV) to the matrices.\n",
    "- We generate a DataFrame with the results for better visualization.\n",
    "- We take the best hyperparameter combination, train an KNN model with the winning hyperparameters and the training set, and then test it with the test set, also generating the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved list from a pickle file\n",
    "with open(\"total_list_stand.pkl\", \"rb\") as list_tot_stand:  # Unpickling\n",
    "    total_list_stand = pickle.load(list_tot_stand)  # Load the data into total_list_stand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lista_total_stand contains 4 lists, each corresponding to one of the preprocessing steps. \n",
    "\n",
    "Each of the 4 lists contains 35 sublists, representing the different segments (chunks) we have generated.\n",
    "\n",
    "Furthermore, each of the 35 sublists contains 19 matrices of size 121x54, one for each EEG channel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid for GridSearchCV\n",
    "grid = {\n",
    "    'n_neighbors': [2, 5, 7, 9],  \n",
    "    'weights': ['uniform', 'distance'],  \n",
    "    'p': [1, 2],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree']\n",
    "}\n",
    "\n",
    "list_results = []  # To store results from each GridSearch\n",
    "cv_results = []  # To store cross-validation results\n",
    "\n",
    "# Loop through all preprocessing types, chunks, and channels\n",
    "for prep in range(4):  # 4 preprocessing types\n",
    "    for chunk in range(35):  # 35 chunks\n",
    "        for chan in range(19):  # 19 channels\n",
    "            print(f'prep: {prep}, chunk: {chunk}, chan: {chan}')\n",
    "            \n",
    "            # Extract features and labels\n",
    "            df_data = total_list_stand[prep][chunk][chan]\n",
    "            data_arr = df_data.iloc[:, :-1].to_numpy()  # Features (all but the last column)\n",
    "            label_arr = df_data['Label'].to_numpy()  # Labels (last column)\n",
    "            \n",
    "            # Initialize the KNN model and GridSearchCV\n",
    "            knn = KNeighborsClassifier()\n",
    "            clf = GridSearchCV(estimator=knn, param_grid=grid, cv=5, return_train_score=True, verbose=2)\n",
    "            \n",
    "            # Fit the model using the full dataset\n",
    "            clf.fit(data_arr, label_arr)\n",
    "            \n",
    "            # Store the model and its CV results\n",
    "            list_results.append(clf)\n",
    "            cv_results.append(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_accuracies = []  # List to store the max accuracy and its corresponding index\n",
    "cont = 0  # Counter to track the index in cv_results\n",
    "\n",
    "# Iterate over the cross-validation results\n",
    "for dic_res in cv_results:\n",
    "    df = pd.DataFrame(dic_res)  # Convert dictionary of results to DataFrame\n",
    "    ind_max = df['mean_test_score'].idxmax()  # Get the index of the highest mean test score\n",
    "    val_max = df.iloc[ind_max]['mean_test_score']  # Get the highest mean test score value\n",
    "    list_accuracies.append((val_max, ind_max, cont))  # Append accuracy, index, and counter to the list\n",
    "    cont += 1  # Increment counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_best_params = []  # List to store the best hyperparameters from GridSearchCV\n",
    "\n",
    "# Iterate over the list of GridSearchCV results\n",
    "for best_p in list_results:\n",
    "    list_best_params.append(best_p.best_params_)  # Append the best hyperparameters from each model\n",
    "\n",
    "# Sort the list of accuracies in descending order\n",
    "list_accuracies.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversor_num_mat(num):\n",
    "    \"\"\"\n",
    "    Convert a numerical position into its corresponding preprocessing, chunk, and channel index.\n",
    "    \"\"\"\n",
    "    chan = num % 19  # Calculate channel index (modulo 19)\n",
    "    n_chunk = math.floor(num / 19)  # Determine chunk index\n",
    "    chunk = n_chunk % 35  # Calculate chunk (modulo 35)\n",
    "    n_prep = math.floor(n_chunk / 35)  # Determine preprocessing index\n",
    "    prep = n_prep % 4  # Calculate preprocessing (modulo 4)\n",
    "    return prep, chunk, chan\n",
    "\n",
    "# Retrieve the best cross-validation result (accuracy and index)\n",
    "dic = cv_results[0]  # Access the first result in cv_results\n",
    "df = pd.DataFrame(dic)  # Convert to DataFrame\n",
    "ind_max = df['mean_test_score'].idxmax()  # Get index of max accuracy\n",
    "val_max = df.iloc[ind_max]['mean_test_score']  # Retrieve the max accuracy\n",
    "\n",
    "list_good_res = []  # List to store the processed results\n",
    "\n",
    "# Loop through the list of accuracies\n",
    "for tup in list_accuracies:\n",
    "    acc = tup[0]  # Extract the accuracy value\n",
    "    pos = tup[2]  # Extract the position of the accuracy in the results list\n",
    "    \n",
    "    # Convert the position into preprocessing, chunk, and channel\n",
    "    prep, chunk, chan = conversor_num_mat(pos)\n",
    "    \n",
    "    # Retrieve the best hyperparameters for the current result\n",
    "    dic = list_results[pos].best_params_\n",
    "    \n",
    "    # Add additional info to the dictionary\n",
    "    dic['Preprocessing'] = prep\n",
    "    dic['Segment'] = chunk\n",
    "    dic['Channel'] = chan\n",
    "    dic['Accuracy'] = acc\n",
    "    \n",
    "    # Append the dictionary to the results list\n",
    "    list_good_res.append(dic)\n",
    "\n",
    "# Convert the results list into a DataFrame for better visualization\n",
    "df_results_knn = pd.DataFrame(list_good_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversor_num_to_chan(num):\n",
    "    \"\"\"\n",
    "    Converts a numeric index to its corresponding EEG channel name.\n",
    "    \"\"\"\n",
    "    # Mapping numeric indices to channel names\n",
    "    channel_map = {\n",
    "        0: 'Fp1', 1: 'Fp2', 2: 'F3', 3: 'F4', 4: 'C3', 5: 'C4', \n",
    "        6: 'P3', 7: 'P4', 8: 'O1', 9: 'O2', 10: 'F7', 11: 'F8',\n",
    "        12: 'T7', 13: 'T8', 14: 'P7', 15: 'P8', 16: 'Fz', 17: 'Cz', 18: 'Pz'\n",
    "    }\n",
    "    \n",
    "    # Return the corresponding channel name or print an error message\n",
    "    return channel_map.get(num, 'error de canales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert channel numbers to their corresponding names using conversor_num_to_chan\n",
    "df_results_knn['Channel'] = [conversor_num_to_chan(i) for i in df_results_knn['Channel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top 20 rows of the KNN results DataFrame\n",
    "df_results_knn.head(20)\n",
    "\n",
    "# Note: These results represent using the entire dataset\n",
    "# 1 segment, 1 sensor (Channel), all features, using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the KNN results DataFrame to a file using pickle\n",
    "with open(\"knn_1channel_df_results\", \"wb\") as f:  # Pickling\n",
    "    pickle.dump(df_results_knn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"knn_1channel_df_results.pkl\", \"wb\") as f:  # Pickling\n",
    "    pickle.dump(df_results_knn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the KNN classifier with specific hyperparameters\n",
    "clas = KNeighborsClassifier(n_neighbors=5, p=1, weights='uniform', algorithm='auto')\n",
    "\n",
    "# Select the specific data (preprocessing=0, chunk=26, channel=16)\n",
    "f_data = total_list_stand[0][12][6]\n",
    "\n",
    "# Convert the data to NumPy arrays for training\n",
    "data_arr = f_data.iloc[:, :-1].to_numpy()  # Features (all columns except the last)\n",
    "label_arr = f_data['Label'].to_numpy()  # Labels (last column)\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_arr, label_arr, \n",
    "                                                    train_size=0.8, random_state=124,\n",
    "                                                    stratify=label_arr)\n",
    "\n",
    "# Train the KNN model\n",
    "clas.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "preds = clas.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(y_test, preds)\n",
    "\n",
    "# Display the confusion matrix\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "features-eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
