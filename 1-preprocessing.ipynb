{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Load the 4 lists of recordings with each of the preprocessing steps (Raw, Filtered, ASR, and ICA)**:\n",
    "   - The recordings are grouped by the preprocessing methods applied to the EEG data: Raw, Filtered, ASR, and ICA. Each list contains the corresponding processed data.\n",
    "\n",
    "2. **Create the segmentation of the EEG recordings (1, 2, 3, 4, 5, 20)**:\n",
    "   - We segment the EEG recordings into multiple time windows or chunks: 1 chunk, 2 chunks, 3 chunks, 4 chunks, 5 chunks, and 20 chunks, to allow feature extraction on smaller time slices.\n",
    "\n",
    "3. **Extract the features for each matrix and save them**:\n",
    "   - For each segmented matrix, we extract various features (e.g., statistical, frequency-domain features) and store them. We process all the 4 preprocessing types, across 35 different matrices (time segments) and for 19 channels.\n",
    "   - This results in `4 x 35 x 19` matrices in total.\n",
    "\n",
    "4. **Standardize the matrices**:\n",
    "   - After extracting the features, we apply standardization to the feature matrices to ensure that all features have zero mean and unit variance.\n",
    "\n",
    "5. **Test the normality, homoscedasticity, and conduct the population contrasts**:\n",
    "   - We apply statistical tests to check for normality and homoscedasticity of the feature data.\n",
    "   - We then split the results into three different lists:\n",
    "     - `significant_diffs`: Contains the cases with statistically significant differences.\n",
    "     - `indeterminate_cases`: Contains the cases where there was indecision in the statistical results.\n",
    "     - `no_significant_diff_cases`: Contains the cases with no statistically significant differences.\n",
    "\n",
    "6. **Analyze the different matrices**:\n",
    "   - We analyze the feature matrices in terms of statistical significance, population contrasts, and other metrics, to extract meaningful patterns from the EEG data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CollectData import CollectData\n",
    "from CreateChunks import CreateChunks\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from CreateFeatures import CreateFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We retrieve the Raw, Filtered, ASR and ICA matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CollectData object\n",
    "collect_data = CollectData()\n",
    "\n",
    "# Load all data (raw, filtered, ASR, ICA)\n",
    "collect_data.load_data()\n",
    "\n",
    "# Retrieve the lists with different preprocessing types\n",
    "# List without preprocessing (raw data)\n",
    "list_raw = collect_data.list_raw\n",
    "# List with filtered data\n",
    "list_filtered = collect_data.list_filtered\n",
    "# List with ASR algorithm applied data\n",
    "list_asr = collect_data.list_asr\n",
    "# List with ICA removed non-brain sources data\n",
    "list_ica = collect_data.list_ica \n",
    "\n",
    "# Save the names of the channels (you already have them manually listed)\n",
    "chan_names = ['Fp1', 'Fp2', 'F3', 'F4', 'C3', 'C4', 'P3', 'P4',\n",
    "              'O1', 'O2', 'F7', 'F8', 'T7', 'T8', 'P7', 'P8', 'Fz', 'Cz', 'Pz']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the raw individuals do not match the Filtered, ASR and ICA, we reordered them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps the index of the filtered data (key) to the corresponding raw data index (value)\n",
    "dict_filt_raw = {}\n",
    "\n",
    "# Iterate through the filtered and raw data lists to find matching shapes\n",
    "for i in range(len(list_filtered)):  # Iterate over the filtered data list\n",
    "    for j in range(len(list_raw)):   # Iterate over the raw data list\n",
    "        if list_filtered[i].shape == list_raw[j].shape:  # Check if the shapes of the datasets match\n",
    "            dict_filt_raw[i] = j  # Store the mapping of filtered index to raw index\n",
    "\n",
    "# Reorder the raw data list based on the new indices (as per filtered data)\n",
    "# Create a new list with the same length as the raw data list\n",
    "reordered_raw_list = [None] * len(list_raw)\n",
    "\n",
    "# Use the dictionary to reorder the raw data\n",
    "for new_index, current_index in dict_filt_raw.items():\n",
    "    reordered_raw_list[new_index] = list_raw[current_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We show graphically the different preprocesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MNE logging level to 'WARNING' to suppress unnecessary messages\n",
    "mne.set_log_level('WARNING')\n",
    "\n",
    "# Define the subject index\n",
    "subj = 1\n",
    "\n",
    "# Create a pandas DataFrame from the raw data for the subject\n",
    "# Each column in the DataFrame corresponds to an EEG channel\n",
    "data = pd.DataFrame(reordered_raw_list[subj], columns=chan_names)\n",
    "\n",
    "# Define the sampling frequency (in Hz)\n",
    "sfreq = 128\n",
    "\n",
    "# Create an MNE Info object, which contains metadata about the EEG channels\n",
    "# ch_names are the names of the channels (column names from the DataFrame)\n",
    "# ch_types is set to 'eeg' since all channels are EEG\n",
    "info = mne.create_info(ch_names=data.columns.to_list(), sfreq=sfreq, ch_types='eeg')\n",
    "\n",
    "# Create a standard EEG montage (standard 10-20 system) for electrode placement\n",
    "montage = mne.channels.make_standard_montage('standard_1020')\n",
    "\n",
    "# Create a RawArray object in MNE using the EEG data and the info object\n",
    "# Data is transposed because MNE expects channels as rows and time points as columns\n",
    "raw = mne.io.RawArray(data.transpose(), info)\n",
    "\n",
    "# Set the montage (standard electrode positions) for the Raw object\n",
    "raw.set_montage(montage)\n",
    "\n",
    "# Uncomment these lines if you want to inspect the raw data and info\n",
    "# print(raw)\n",
    "# print(raw.info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a function which, starting from the subject, gives us the 4 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def info_data(data_type):\n",
    "    \"\"\"\n",
    "    Creates an MNE RawArray object from a pandas DataFrame containing EEG data.\n",
    "    \n",
    "    Args:\n",
    "        data_type (pd.DataFrame): A DataFrame containing EEG data where columns are channels.\n",
    "        \n",
    "    Returns:\n",
    "        raw (mne.io.RawArray): An MNE RawArray object containing the EEG data with the appropriate montage and info.\n",
    "    \"\"\"\n",
    "    # Sampling frequency (in Hz)\n",
    "    sfreq = 128\n",
    "    \n",
    "    # Create MNE Info object with channel names and sampling frequency\n",
    "    info = mne.create_info(ch_names=data_type.columns.to_list(), sfreq=sfreq, ch_types='eeg')\n",
    "    \n",
    "    # Create a standard EEG montage (10-20 electrode system)\n",
    "    montage = mne.channels.make_standard_montage('standard_1020')\n",
    "    \n",
    "    # Create an MNE RawArray using the transposed data (MNE expects channels as rows, time as columns)\n",
    "    raw = mne.io.RawArray(data_type.transpose(), info)\n",
    "    \n",
    "    # Set the montage (electrode positions) for the RawArray object\n",
    "    raw.set_montage(montage)\n",
    "    \n",
    "    return raw\n",
    "\n",
    "def plots_preprocessing_subject(subj):\n",
    "    \"\"\"\n",
    "    Loads the raw, filtered, ASR-filtered, and ICA-filtered data for a specific subject,\n",
    "    creates MNE RawArray objects for each type of data, and returns them.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing RawArray objects for raw, filtered, ASR-filtered, and ICA-filtered data.\n",
    "    \"\"\"\n",
    "    # Load raw data for the specific subject and create RawArray\n",
    "    data_raw = pd.DataFrame(reordered_raw_list[subj], columns=chan_names)\n",
    "    raw_raw = info_data(data_raw)\n",
    "    \n",
    "    # Load filtered data for the specific subject and create RawArray\n",
    "    data_filt = pd.DataFrame(list_filtered[subj], columns=chan_names)\n",
    "    raw_filt = info_data(data_filt)\n",
    "    \n",
    "    # Load ASR-filtered data for the specific subject and create RawArray\n",
    "    data_asr = pd.DataFrame(list_asr[subj], columns=chan_names)\n",
    "    raw_asr = info_data(data_asr)\n",
    "    \n",
    "    # Load ICA-filtered data for the specific subject and create RawArray\n",
    "    data_ica = pd.DataFrame(list_ica[subj], columns=chan_names)\n",
    "    raw_ica = info_data(data_ica)\n",
    "    \n",
    "    return raw_raw, raw_filt, raw_asr, raw_ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a plot of what the raw data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.DataFrame(list_ica[71],columns=chan_names)\n",
    "raw_ica = info_data(data_raw)\n",
    "fig = raw_ica.plot(color='#5AB0B0',scalings=dict(eeg=600), duration=100,\\\n",
    "             show_scrollbars=False, show_scalebars=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and preprocess data for a specific subject (subject 71)\n",
    "raw_raw, raw_filt, raw_asr, raw_ica = plots_preprocessing_subject(71)\n",
    "\n",
    "# Frequency range for TFR computation (Morlet wavelet method)\n",
    "freqs = np.arange(0.5, 60, 2)  # Frequency range from 0.5 to 60 Hz in 2 Hz steps\n",
    "\n",
    "# Common parameters for plotting\n",
    "plot_params = {\n",
    "    'picks': ['Fp1'],  # Select the Fp1 channel for visualization\n",
    "    'cmap': 'jet',  # Use the 'jet' color map\n",
    "    'dB': True,  # Convert power to decibels\n",
    "    'vmin': 0,  # Minimum value for color scaling\n",
    "    'vmax': 80,  # Maximum value for color scaling\n",
    "    'tmax': 80  # Maximum time to plot (in seconds)\n",
    "}\n",
    "\n",
    "# Compute and plot TFR for raw data (units = V^2/Hz)\n",
    "power_raw = raw_raw.compute_tfr(method='morlet', freqs=freqs, picks=chan_names)\n",
    "power_raw.plot(**plot_params)\n",
    "\n",
    "# Compute and plot TFR for filtered data (units = V^2/Hz)\n",
    "power_filt = raw_filt.compute_tfr(method='morlet', freqs=freqs, picks=chan_names)\n",
    "power_filt.plot(**plot_params)\n",
    "\n",
    "# Compute and plot TFR for ASR-filtered data (units = V^2/Hz)\n",
    "power_asr = raw_asr.compute_tfr(method='morlet', freqs=freqs, picks=chan_names)\n",
    "power_asr.plot(**plot_params)\n",
    "\n",
    "# Compute and plot TFR for ICA-filtered data (units = V^2/Hz)\n",
    "power_ica = raw_ica.compute_tfr(method='morlet', freqs=freqs, picks=chan_names)\n",
    "power_ica.plot(**plot_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data for subject 71 with different preprocessing methods\n",
    "raw_raw, raw_filt, raw_asr, raw_ica = plots_preprocessing_subject(71)\n",
    "\n",
    "# List of raw data objects and corresponding titles for better identification in the plots\n",
    "data_list = [\n",
    "    (raw_raw, \"Raw Data\"),\n",
    "    (raw_filt, \"Filtered Data\"),\n",
    "    (raw_asr, \"ASR Data\"),\n",
    "    (raw_ica, \"ICA Data\")\n",
    "]\n",
    "\n",
    "# Loop over the data and plot the PSD for each preprocessing step\n",
    "for raw_data, title in data_list:\n",
    "    raw_data.plot_psd(fmax=55, show=False)  # Plot up to 55 Hz\n",
    "    plt.title(title)  # Add title to each plot for clarity\n",
    "    plt.show()  # Display each plot individually\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We split the matrices into segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the CreateChunks object to create chunks\n",
    "create_chunks = CreateChunks()\n",
    "\n",
    "# Create lists that will contain the chunks for each type of preprocessed data\n",
    "# Each call to get_more_chunks splits the data into chunks with sizes [1, 2, 3, 4, 5, 20]\n",
    "list_chunks_raw = create_chunks.get_more_chunks(list_raw)\n",
    "list_chunks_filtered = create_chunks.get_more_chunks(list_filtered)\n",
    "list_chunks_asr = create_chunks.get_more_chunks(list_asr)\n",
    "list_chunks_ica = create_chunks.get_more_chunks(list_ica)\n",
    "\n",
    "# Explanation of the structure:\n",
    "# - Each of the above lists (lista_chunks_raw, lista_chunks_filtered, etc.) contains 6 elements.\n",
    "# - These 6 elements correspond to the different chunk sizes: [1, 2, 3, 4, 5, 20].\n",
    "# - Inside each of these elements, there are 121 subjects (or individuals).\n",
    "# - For each subject, there's a 2D array of shape (time x channels), representing the EEG data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of the EEG features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_matrix(list_data):\n",
    "    \"\"\"\n",
    "    This function extracts features from multiple EEG data chunks and saves each \n",
    "    chunk's features into a .npy file. The features are computed for each chunk \n",
    "    and stored in a final list.\n",
    "\n",
    "    Args:\n",
    "        list_data (list of lists): A list containing multiple chunks of EEG data \n",
    "                                   (each chunk is a list of individual windows).\n",
    "\n",
    "    Returns:\n",
    "        final_list (list): A list of numpy arrays, each containing the features extracted\n",
    "                           for a chunk of EEG data.\n",
    "    \"\"\"\n",
    "    # Instantiate the feature extraction object\n",
    "    createFeatures = CreateFeatures()\n",
    "    final_list = []  # List to store the feature matrices\n",
    "    \n",
    "    # Loop over the chunks (1-6 as per your chunk sizes)\n",
    "    for chunk_idx, chunks in enumerate(list_data):\n",
    "        print(f'Processing chunk group {chunk_idx + 1} of {len(list_data)}')\n",
    "\n",
    "        # Loop over each chunk in the chunk group (for example, [1, 2, 3, 4, 5, 20])\n",
    "        for chunk_inner_idx, chunk in enumerate(chunks):\n",
    "            print(f'Processing inner chunk {chunk_inner_idx + 1} of {len(chunks)}')\n",
    "\n",
    "            # Extract features for the current chunk\n",
    "            array_data = createFeatures.get_features(chunk)\n",
    "\n",
    "            # Get the current time to create a unique filename\n",
    "            current_time = datetime.now().strftime(\"%d-%H-%M-%S\")\n",
    "            \n",
    "            # Save the feature matrix as a .npy file\n",
    "            np.save(f'array_features_{current_time}.npy', array_data)\n",
    "            \n",
    "            # Append the feature matrix to the final list\n",
    "            final_list.append(array_data)\n",
    "    \n",
    "    return final_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Since this process is very resource-intensive, we decided to periodically save the feature arrays as they were being generated. In the next step, these arrays are read and combined into a single list, which we call `total_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of preprocessed data chunks for each preprocessing type\n",
    "# data_chunks_list = [\n",
    "#     ('raw', list_chunks_raw),\n",
    "#     ('filtered', list_chunks_filtered),\n",
    "#     ('asr', list_chunks_asr),\n",
    "#     ('ica', list_chunks_ica)\n",
    "# ]\n",
    "\n",
    "# # Dictionary to store the resulting feature lists for each preprocessing type\n",
    "# feature_lists = {}\n",
    "\n",
    "# # Loop over the data chunks and process each\n",
    "# for name, data_chunks in data_chunks_list:\n",
    "#     print(f\"Processing features for {name} data\")\n",
    "#     feature_lists[name] = create_features_matrix(data_chunks)\n",
    "\n",
    "# # Now `feature_lists` contains the feature lists for 'raw', 'filtered', 'asr', and 'ica'\n",
    "# # You can access them as follows:\n",
    "# lists_feat_raw = feature_lists['raw']\n",
    "# lists_feat_filtered = feature_lists['filtered']\n",
    "# lists_feat_asr = feature_lists['asr']\n",
    "# lists_feat_ica = feature_lists['ica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data list (raw, filtered, ASR, and ICA), we have 35 matrices. Each matrix contains the features of 121 individuals, extracted from different segments of the recording:\n",
    "\n",
    "1. 100% of the recording (1/1)\n",
    "2. First half of the recording (1st/2)\n",
    "3. Second half of the recording (2nd/2)\n",
    "4. First third of the recording (1st/3)\n",
    "5. Second third of the recording (2nd/3)\n",
    "6. Third third of the recording (3rd/3)\n",
    "...\n",
    "33. Eighteenth twentieth of the recording (18th/20)\n",
    "34. Nineteenth twentieth of the recording (19th/20)\n",
    "35. Twentieth twentieth of the recording (20th/20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We read the saved arrays\n",
    "#### It is not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists_feat_raw=['array_features/array_features_06-10-11-28.npy', 'array_features/array_features_06-10-23-37.npy',\n",
    "#                'array_features/array_features_06-10-35-34.npy', 'array_features/array_features_06-10-41-49.npy',\n",
    "#                'array_features/array_features_06-10-47-55.npy', 'array_features/array_features_06-10-53-57.npy',\n",
    "#                'array_features/array_features_06-10-57-54.npy', 'array_features/array_features_06-11-01-50.npy', \n",
    "#                'array_features/array_features_06-11-05-42.npy', 'array_features/array_features_06-11-09-29.npy', \n",
    "#                'array_features/array_features_06-11-12-12.npy', 'array_features/array_features_06-11-14-55.npy', \n",
    "#                'array_features/array_features_06-11-17-40.npy', 'array_features/array_features_06-11-20-24.npy', \n",
    "#                'array_features/array_features_06-11-23-07.npy', 'array_features/array_features_06-11-23-32.npy', \n",
    "#                'array_features/array_features_06-11-23-56.npy', 'array_features/array_features_06-11-24-19.npy', \n",
    "#                'array_features/array_features_06-11-24-42.npy', 'array_features/array_features_06-11-25-05.npy', \n",
    "#                'array_features/array_features_06-11-25-28.npy', 'array_features/array_features_06-11-25-51.npy', \n",
    "#                'array_features/array_features_06-11-26-15.npy', 'array_features/array_features_06-11-26-38.npy', \n",
    "#                'array_features/array_features_06-11-27-01.npy', 'array_features/array_features_06-11-27-24.npy', \n",
    "#                'array_features/array_features_06-11-27-47.npy', 'array_features/array_features_06-11-28-10.npy', \n",
    "#                'array_features/array_features_06-11-28-33.npy', 'array_features/array_features_06-11-28-56.npy', \n",
    "#                'array_features/array_features_06-11-29-19.npy', 'array_features/array_features_06-11-29-42.npy', \n",
    "#                'array_features/array_features_06-11-30-04.npy', 'array_features/array_features_06-11-30-27.npy', \n",
    "#                'array_features/array_features_06-11-30-50.npy']\n",
    "\n",
    "# lists_feat_fil=['array_features/array_features_06-12-16-45.npy', 'array_features/array_features_06-12-30-31.npy',\n",
    "#                'array_features/array_features_06-12-44-03.npy', 'array_features/array_features_06-12-51-01.npy',\n",
    "#                'array_features/array_features_06-12-57-51.npy', 'array_features/array_features_06-13-04-41.npy',\n",
    "#                'array_features/array_features_06-13-09-04.npy', 'array_features/array_features_06-13-13-24.npy', \n",
    "#                'array_features/array_features_06-13-17-42.npy', 'array_features/array_features_06-13-21-58.npy', \n",
    "#                'array_features/array_features_06-13-25-01.npy', 'array_features/array_features_06-13-28-01.npy', \n",
    "#                'array_features/array_features_06-13-31-01.npy', 'array_features/array_features_06-13-34-00.npy', \n",
    "#                'array_features/array_features_06-13-36-59.npy', 'array_features/array_features_06-13-37-23.npy', \n",
    "#                'array_features/array_features_06-13-37-48.npy', 'array_features/array_features_06-13-38-12.npy', \n",
    "#                'array_features/array_features_06-13-38-37.npy', 'array_features/array_features_06-13-39-01.npy', \n",
    "#                'array_features/array_features_06-13-39-25.npy', 'array_features/array_features_06-13-39-50.npy', \n",
    "#                'array_features/array_features_06-13-40-14.npy', 'array_features/array_features_06-13-40-38.npy', \n",
    "#                'array_features/array_features_06-13-41-03.npy', 'array_features/array_features_06-13-41-27.npy', \n",
    "#                'array_features/array_features_06-13-41-51.npy', 'array_features/array_features_06-13-42-15.npy', \n",
    "#                'array_features/array_features_06-13-42-40.npy', 'array_features/array_features_06-13-43-04.npy', \n",
    "#                'array_features/array_features_06-13-43-28.npy', 'array_features/array_features_06-13-43-53.npy', \n",
    "#                'array_features/array_features_06-13-44-17.npy', 'array_features/array_features_06-13-44-41.npy', \n",
    "#                'array_features/array_features_06-13-45-05.npy']\n",
    "\n",
    "# lists_feat_asr=['array_features/array_features_06-14-27-03.npy', 'array_features/array_features_06-14-40-07.npy',\n",
    "#                'array_features/array_features_06-14-53-09.npy', 'array_features/array_features_06-14-59-43.npy',\n",
    "#                'array_features/array_features_06-15-06-50.npy', 'array_features/array_features_06-15-15-31.npy',\n",
    "#                'array_features/array_features_06-15-20-58.npy', 'array_features/array_features_06-15-26-25.npy', \n",
    "#                'array_features/array_features_06-15-31-50.npy', 'array_features/array_features_06-15-36-47.npy', \n",
    "#                'array_features/array_features_06-15-39-42.npy', 'array_features/array_features_06-15-42-37.npy', \n",
    "#                'array_features/array_features_06-15-45-31.npy', 'array_features/array_features_06-15-48-25.npy', \n",
    "#                'array_features/array_features_06-15-51-18.npy', 'array_features/array_features_06-15-51-43.npy', \n",
    "#                'array_features/array_features_06-15-52-08.npy', 'array_features/array_features_06-15-52-32.npy', \n",
    "#                'array_features/array_features_06-15-52-57.npy', 'array_features/array_features_06-15-53-21.npy', \n",
    "#                'array_features/array_features_06-15-53-46.npy', 'array_features/array_features_06-15-54-10.npy', \n",
    "#                'array_features/array_features_06-15-54-35.npy', 'array_features/array_features_06-15-54-59.npy', \n",
    "#                'array_features/array_features_06-15-55-23.npy', 'array_features/array_features_06-15-55-48.npy', \n",
    "#                'array_features/array_features_06-15-56-12.npy', 'array_features/array_features_06-15-56-36.npy', \n",
    "#                'array_features/array_features_06-15-57-01.npy', 'array_features/array_features_06-15-57-25.npy', \n",
    "#                'array_features/array_features_06-15-57-50.npy', 'array_features/array_features_06-15-58-14.npy', \n",
    "#                'array_features/array_features_06-15-58-39.npy', 'array_features/array_features_06-15-59-03.npy',\n",
    "#                'array_features/array_features_06-15-59-27.npy']  \n",
    "\n",
    "# lists_feat_ica=['array_features/array_features_06-16-44-17.npy', 'array_features/array_features_06-17-00-47.npy',\n",
    "#                'array_features/array_features_06-17-17-17.npy', 'array_features/array_features_06-17-25-40.npy',\n",
    "#                'array_features/array_features_06-17-33-38.npy', 'array_features/array_features_06-17-40-05.npy',\n",
    "#                'array_features/array_features_06-17-44-07.npy', 'array_features/array_features_06-17-48-09.npy', \n",
    "#                'array_features/array_features_06-17-52-10.npy', 'array_features/array_features_06-17-56-12.npy', \n",
    "#                'array_features/array_features_06-17-59-01.npy', 'array_features/array_features_06-18-01-50.npy', \n",
    "#                'array_features/array_features_06-18-04-39.npy', 'array_features/array_features_06-18-07-29.npy', \n",
    "#                'array_features/array_features_06-18-10-18.npy', 'array_features/array_features_06-18-10-42.npy', \n",
    "#                'array_features/array_features_06-18-11-06.npy', 'array_features/array_features_06-18-11-30.npy', \n",
    "#                'array_features/array_features_06-18-11-54.npy', 'array_features/array_features_06-18-12-18.npy', \n",
    "#                'array_features/array_features_06-18-12-42.npy', 'array_features/array_features_06-18-13-06.npy', \n",
    "#                'array_features/array_features_06-18-13-30.npy', 'array_features/array_features_06-18-13-54.npy', \n",
    "#                'array_features/array_features_06-18-14-18.npy', 'array_features/array_features_06-18-14-43.npy', \n",
    "#                'array_features/array_features_06-18-15-07.npy', 'array_features/array_features_06-18-15-31.npy', \n",
    "#                'array_features/array_features_06-18-15-55.npy', 'array_features/array_features_06-18-16-19.npy', \n",
    "#                'array_features/array_features_06-18-16-43.npy', 'array_features/array_features_06-18-17-07.npy', \n",
    "#                'array_features/array_features_06-18-17-31.npy', 'array_features/array_features_06-18-17-55.npy', \n",
    "#                'array_features/array_features_06-18-18-19.npy',]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_npy(file_list):\n",
    "#     \"\"\"\n",
    "#     Loads multiple .npy files from a list of filenames and returns them as a list of arrays.\n",
    "\n",
    "#     Args:\n",
    "#         file_list (list of str): List of filenames (paths) to the .npy files to be loaded.\n",
    "\n",
    "#     Returns:\n",
    "#         list of numpy arrays: A list containing the numpy arrays loaded from the provided filenames.\n",
    "#     \"\"\"\n",
    "#     loaded_arrays = []\n",
    "    \n",
    "#     # Loop over each filename in the provided list\n",
    "#     for filename in file_list:\n",
    "#         try:\n",
    "#             # Load the .npy file and append the resulting array to the list\n",
    "#             array = np.load(filename)\n",
    "#             loaded_arrays.append(array)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading {filename}: {e}\")\n",
    "    \n",
    "#     return loaded_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an empty list to store all loaded arrays\n",
    "# total_list = []\n",
    "\n",
    "# # Loop through each list of feature files (raw, filtered, ASR, and ICA)\n",
    "# for feature_list in [lists_feat_raw, lists_feat_fil, lists_feat_asr, lists_feat_ica]:\n",
    "#     # Load the numpy arrays from each list of filenames and append them to total_list\n",
    "#     loaded_features = load_npy(feature_list)\n",
    "#     total_list.append(loaded_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving the total_list to a file using pickle\n",
    "# with open('total_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(total_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We load the saved list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the total_list from the saved file\n",
    "with open('total_list.pkl', 'rb') as f:\n",
    "    total_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardisation of matrices\n",
    "\n",
    "We need to standardise the data by columns for the test to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the standardized data for all preprocessing types\n",
    "total_list_stand = []  # 4 lists (raw, filtered, ASR, ICA)\n",
    "\n",
    "# Loop through each preprocessed list in total_list (raw, filtered, ASR, ICA)\n",
    "for list_prep in total_list:\n",
    "    lista_trozos = []  # This will store the 35 matrices for each preprocessed type\n",
    "\n",
    "    # Loop through the 35 matrices inside each preprocessed list\n",
    "    for mat in list_prep:\n",
    "        lista_canales = []  # List to store standardized data for 19 channels\n",
    "        \n",
    "        # Standardize each channel (121x53 for each channel)\n",
    "        for chan in range(19):\n",
    "            # Convert to DataFrame for easier manipulation and standardization\n",
    "            df = pd.DataFrame(mat[chan])\n",
    "            \n",
    "            # Standardize the data using StandardScaler\n",
    "            scaler_standard = StandardScaler()\n",
    "            arr_stand = scaler_standard.fit_transform(df)\n",
    "\n",
    "            # Convert the standardized data back to a DataFrame\n",
    "            df_stand = pd.DataFrame(arr_stand)\n",
    "\n",
    "            # Add the label column (1 for subjects < 61, and 0 for subjects >= 61)\n",
    "            labels = [1 if x < 61 else 0 for x in range(121)]\n",
    "            df_stand['Label'] = labels  # Add the labels as a new column\n",
    "\n",
    "            # Rename the columns for clarity (53 feature columns + 1 label column)\n",
    "            column_names = [f'Feature_{x}' for x in range(53)]\n",
    "            column_names.append('Label')\n",
    "            df_stand.columns = column_names\n",
    "\n",
    "            # Append the standardized DataFrame for this channel to lista_canales\n",
    "            lista_canales.append(df_stand)\n",
    "\n",
    "        # Append the list of standardized channels for this matrix (35 matrices) to lista_trozos\n",
    "        lista_trozos.append(lista_canales)\n",
    "\n",
    "    # Append the processed data for each preprocessing type (raw, filtered, ASR, ICA) to total_list_stand\n",
    "    total_list_stand.append(lista_trozos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the total_list_stand to a file using pickle\n",
    "with open('total_list_stand.pkl', 'wb') as f:\n",
    "    pickle.dump(total_list_stand, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have 4 main lists in total_list (raw, filtered, ASR, ICA). For each, you process 35 matrices, each representing different divisions of the recordings (e.g., 1/1, 1/2, etc.).\n",
    "Within each matrix, there are 19 channels, and you standardize the data for each channel (121x53 matrix) using StandardScaler.\n",
    "After standardization, you add a label for each subject (121 rows), indicating whether they belong to group 1 or 0, and then you append the standardized data to total_list_stand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now total_list_stand contains standardized features with labels for each preprocessing type:\n",
    "\n",
    "- total_list_stand[0] -> Standardized data for raw preprocessing\n",
    "- total_list_stand[1] -> Standardized data for filtered preprocessing\n",
    "- total_list_stand[2] -> Standardized data for ASR preprocessing\n",
    "- total_list_stand[3] -> Standardized data for ICA preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, total_list_stand contains 4 lists, one for each preprocessing type. Each of these 4 lists contains 35 sublists, representing the different segments we generated. Within each of these 35 sublists, there are 19 matrices (one per channel), each with dimensions 121x54."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality test, test of variances and population contrast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Counters for different cases (sum total = 2660)\n",
    "count_norm_homo = 0  # Normal distribution, homogeneous variances\n",
    "count_norm_hete = 0  # Normal distribution, heterogeneous variances\n",
    "count_non_norm_homo = 0  # Non-normal distribution, homogeneous variances\n",
    "count_non_norm_hete = 0  # Non-normal distribution, heterogeneous variances\n",
    "count_sig_diff = 0  # Significant differences between groups\n",
    "count_no_sig_diff = 0  # No significant differences between groups\n",
    "\n",
    "# Lists to store tuples with significant differences (i, j, k, s)\n",
    "# (preprocessing, segment, channel, feature)\n",
    "significant_diffs = []\n",
    "indeterminate_cases = []\n",
    "no_significant_diff_cases = []\n",
    "\n",
    "# Iterate over the preprocessing types\n",
    "for preprocessing_idx in range(4):\n",
    "    # Iterate over the recording segments\n",
    "    for segment_idx in range(35):\n",
    "        # Iterate over the channels\n",
    "        for channel_idx in range(19):\n",
    "            # Iterate over the features\n",
    "            df_features = total_list_stand[preprocessing_idx][segment_idx][channel_idx]\n",
    "            for feature_idx in range(53):\n",
    "                # Separate populations (ADHD vs Control)\n",
    "                adhd_group = df_features[df_features['Label'] == 1]\n",
    "                control_group = df_features[df_features['Label'] == 0]\n",
    "                \n",
    "                # Normality test\n",
    "                pval_control = stats.normaltest(control_group.iloc[:, feature_idx]).pvalue\n",
    "                pval_adhd = stats.normaltest(adhd_group.iloc[:, feature_idx]).pvalue\n",
    "\n",
    "                if (pval_adhd < 0.05) or (pval_control < 0.05):\n",
    "                    # Non-normal samples: Levene's test for variance\n",
    "                    pval_var = stats.levene(adhd_group.iloc[:, feature_idx], control_group.iloc[:, feature_idx]).pvalue\n",
    "\n",
    "                    if pval_var < 0.05:\n",
    "                        # Heterogeneous variances: indeterminate case\n",
    "                        count_non_norm_hete += 1\n",
    "                        indeterminate_cases.append((preprocessing_idx, segment_idx, channel_idx, feature_idx))\n",
    "                    else:\n",
    "                        # Homogeneous variances: Perform T-test or Mann-Whitney test\n",
    "                        pval_ttest = stats.ttest_ind(adhd_group.iloc[:, feature_idx], control_group.iloc[:, feature_idx]).pvalue\n",
    "                        count_non_norm_homo += 1\n",
    "                        if pval_ttest > 0.05:\n",
    "                            # No significant differences -> Means are equal\n",
    "                            count_no_sig_diff += 1\n",
    "                            no_significant_diff_cases.append((preprocessing_idx, segment_idx, channel_idx, feature_idx))\n",
    "                        else:\n",
    "                            # Significant differences exist\n",
    "                            count_sig_diff += 1\n",
    "                            significant_diffs.append((preprocessing_idx, segment_idx, channel_idx, feature_idx))\n",
    "                else:\n",
    "                    # Normal samples: Bartlett's test for variance\n",
    "                    pval_var = stats.bartlett(adhd_group.iloc[:, feature_idx], control_group.iloc[:, feature_idx]).pvalue\n",
    "\n",
    "                    if pval_var < 0.05:\n",
    "                        # Heterogeneous variances\n",
    "                        count_norm_hete += 1\n",
    "                        pval_ttest = stats.ttest_ind(adhd_group.iloc[:, feature_idx], control_group.iloc[:, feature_idx], equal_var=False).pvalue\n",
    "                        if pval_ttest > 0.05:\n",
    "                            # No significant differences -> Means are equal\n",
    "                            count_no_sig_diff += 1\n",
    "                            no_significant_diff_cases.append((preprocessing_idx, segment_idx, channel_idx, feature_idx))\n",
    "                        else:\n",
    "                            # Significant differences exist\n",
    "                            count_sig_diff += 1\n",
    "                            significant_diffs.append((preprocessing_idx, segment_idx, channel_idx, feature_idx))\n",
    "                    else:\n",
    "                        # Homogeneous variances: T-test\n",
    "                        count_norm_homo += 1\n",
    "                        pval_ttest = stats.ttest_ind(adhd_group.iloc[:, feature_idx], control_group.iloc[:, feature_idx]).pvalue\n",
    "                        if pval_ttest > 0.05:\n",
    "                            # No significant differences -> Means are equal\n",
    "                            count_no_sig_diff += 1\n",
    "                            no_significant_diff_cases.append((preprocessing_idx, segment_idx, channel_idx, feature_idx))\n",
    "                        else:\n",
    "                            # Significant differences exist\n",
    "                            count_sig_diff += 1\n",
    "                            significant_diffs.append((preprocessing_idx, segment_idx, channel_idx, feature_idx))\n",
    "\n",
    "# Total cases calculated\n",
    "total_cases = count_norm_homo + count_norm_hete + count_non_norm_homo + count_non_norm_hete\n",
    "indeterminate_cases_count = total_cases - count_no_sig_diff - count_sig_diff\n",
    "\n",
    "# Output results\n",
    "print('Total case count: ', total_cases)\n",
    "print('Significant difference case count:', count_sig_diff)\n",
    "print('No significant difference case count:', count_no_sig_diff, len(no_significant_diff_cases))\n",
    "print('Cases with non-normal distribution and heterogeneous variance:', indeterminate_cases_count, len(indeterminate_cases))\n",
    "print('Length of significant difference cases list:', len(significant_diffs))\n",
    "print('Indeterminate significant cases:', count_sig_diff - len(significant_diffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the statistically significant cases\n",
    "df_significant = pd.DataFrame(significant_diffs)\n",
    "\n",
    "# Define column names to clearly represent what each column contains\n",
    "columns = ['Preprocessing', 'Segment', 'Channel', 'Feature']\n",
    "df_significant.columns = columns\n",
    "\n",
    "# Plot the distribution of significant cases across preprocessing types\n",
    "df_significant['preprocessing'].value_counts().plot(kind='bar')\n",
    "\n",
    "# Display the bar plot\n",
    "plt.xlabel('Preprocessing Type')\n",
    "plt.ylabel('Count of Significant Cases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the indeterminate cases\n",
    "df_indeterminate_cases = pd.DataFrame(indeterminate_cases)\n",
    "\n",
    "# Define column names for the DataFrame: preprocessing step, segment, channel, and feature\n",
    "column_names = ['Preprocessing', 'Segment', 'Channel', 'Feature']\n",
    "df_indeterminate_cases.columns = column_names\n",
    "\n",
    "# Plot the counts of indeterminate cases per preprocessing type\n",
    "df_indeterminate_cases['Preprocessing'].value_counts().plot(kind='bar', title='Indeterminate Cases by Preprocessing Type')\n",
    "\n",
    "# Display the bar plot\n",
    "plt.xlabel('Preprocessing Type')\n",
    "plt.ylabel('Count of Indeterminate Cases')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with the no significant difference cases\n",
    "df_no_significant_diff = pd.DataFrame(no_significant_diff_cases)\n",
    "\n",
    "# Define column names for the DataFrame: preprocessing step, segment, channel, and feature\n",
    "column_names = ['Preprocessing', 'Segment', 'Channel', 'Feature']\n",
    "df_no_significant_diff.columns = column_names\n",
    "\n",
    "# Plot the counts of no significant difference cases per preprocessing type\n",
    "df_no_significant_diff['Preprocessing'].value_counts().plot(kind='bar', title='No Significant Difference Cases by Preprocessing Type')\n",
    "\n",
    "# Display the bar plot\n",
    "plt.xlabel('Preprocessing Type')\n",
    "plt.ylabel('Count of No Significant Difference Cases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of statistically significant cases per segment\n",
    "segment_counts = df_significant['Segment'].value_counts().sort_index()\n",
    "segments = segment_counts.index\n",
    "x_pos = np.arange(len(segments))\n",
    "\n",
    "# Define custom colors for each segment\n",
    "colors = ['black', 'red', 'red', 'green', 'green', 'green',\n",
    "          'blue', 'blue', 'blue', 'blue', 'cyan', 'cyan',\n",
    "          'cyan', 'cyan', 'cyan', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow']\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(x_pos, segment_counts, color=colors)\n",
    "\n",
    "# Set the x-ticks and their labels\n",
    "plt.xticks(x_pos, segments)\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('Recording Segment (Trozo)')\n",
    "plt.ylabel('Count of Statistically Significant Cases')\n",
    "plt.title('Statistically Significant Cases by Segment')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of indeterminate cases per segment\n",
    "segment_counts = df_indeterminate_cases['Segment'].value_counts().sort_index()\n",
    "segments = segment_counts.index\n",
    "x_pos = np.arange(len(segments))\n",
    "\n",
    "# Define custom colors for each segment\n",
    "colors = ['black', 'red', 'red', 'green', 'green', 'green',\n",
    "          'blue', 'blue', 'blue', 'blue', 'cyan', 'cyan',\n",
    "          'cyan', 'cyan', 'cyan', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow']\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(x_pos, segment_counts, color=colors)\n",
    "\n",
    "# Set the x-ticks and their labels\n",
    "plt.xticks(x_pos, segments)\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('Recording Segment')\n",
    "plt.ylabel('Count of Indeterminate Cases')\n",
    "plt.title('Indeterminate Cases by Segment')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the count of \"no significant difference\" cases per segment\n",
    "segment_counts = df_no_significant_diff['Segment'].value_counts().sort_index()\n",
    "segments = segment_counts.index\n",
    "x_pos = np.arange(len(segments))\n",
    "\n",
    "# Define custom colors for each segment\n",
    "colors = ['black', 'red', 'red', 'green', 'green', 'green',\n",
    "          'blue', 'blue', 'blue', 'blue', 'cyan', 'cyan',\n",
    "          'cyan', 'cyan', 'cyan', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow', 'yellow', 'yellow', \n",
    "          'yellow', 'yellow', 'yellow']\n",
    "\n",
    "# Create the bar plot\n",
    "plt.bar(x_pos, segment_counts, color=colors)\n",
    "\n",
    "# Set the x-ticks and their labels\n",
    "plt.xticks(x_pos, segments)\n",
    "\n",
    "# Add labels and title to the plot\n",
    "plt.xlabel('Recording Segment (Trozo)')\n",
    "plt.ylabel('Count of No Significant Difference Cases')\n",
    "plt.title('No Significant Difference Cases by Segment')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping of channel indices to channel names\n",
    "channel_mapping = {\n",
    "    0: 'Fp1', 1: 'Fp2', 2: 'F3', 3: 'F4', 4: 'C3', 5: 'C4',\n",
    "    6: 'P3', 7: 'P4', 8: 'O1', 9: 'O2', 10: 'F7', 11: 'F8',\n",
    "    12: 'T7', 13: 'T8', 14: 'P7', 15: 'P8', 16: 'Fz', 17: 'Cz', 18: 'Pz'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of significant cases per channel\n",
    "channel_counts = df_significant['Channel'].value_counts().sort_index()\n",
    "\n",
    "# Replace channel indices with their corresponding names\n",
    "channel_counts.index = channel_counts.index.map(channel_mapping)\n",
    "\n",
    "# Plot the bar chart with channel names on the x-axis\n",
    "channel_counts.plot(kind='bar', title='Significant Cases by Channel')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Channel')\n",
    "plt.ylabel('Count of Significant Cases')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of indeterminate cases per channel\n",
    "channel_counts = df_indeterminate_cases['Channel'].value_counts().sort_index()\n",
    "\n",
    "# Replace channel indices with their corresponding names\n",
    "channel_counts.index = channel_counts.index.map(channel_mapping)\n",
    "\n",
    "# Plot the bar chart with channel names on the x-axis\n",
    "channel_counts.plot(kind='bar', title='Indeterminate Cases by Channel')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Channel')\n",
    "plt.ylabel('Count of Indeterminate Cases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of \"no significant difference\" cases per channel\n",
    "channel_counts = df_no_significant_diff['Channel'].value_counts().sort_index()\n",
    "\n",
    "# Replace channel indices with their corresponding names\n",
    "channel_counts.index = channel_counts.index.map(channel_mapping)\n",
    "\n",
    "# Plot the bar chart with channel names on the x-axis\n",
    "channel_counts.plot(kind='bar', title='No Significant Difference Cases by Channel')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Channel')\n",
    "plt.ylabel('Count of No Significant Difference Cases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping of column indices to feature names\n",
    "feature_mapping = {\n",
    "    0: 'mean', 1: 'variance', 2: 'std', 3: 'ptp_amp', 4: 'skewness', 5: 'kurtosis', \n",
    "    6: 'rms', 7: 'quantile', 8: 'hurst_exp', 9: 'app_entropy', 10: 'decorr_time', \n",
    "    11: 'pow_freq_bands_1', 12: 'pow_freq_bands_2', 13: 'pow_freq_bands_3', 14: 'pow_freq_bands_4', \n",
    "    15: 'hjorth_mobility_spect', 16: 'hjorth_complexity_spect', 17: 'hjorth_mobility', \n",
    "    18: 'hjorth_complexity', 19: 'higuchi_fd', 20: 'katz_fd', 21: 'zero_crossings', \n",
    "    22: 'line_length', 23: 'spect_slope_1', 24: 'spect_slope_2', 25: 'spect_slope_3', \n",
    "    26: 'spect_slope_4', 27: 'spect_entropy', 28: 'energy_freq_bands_1', \n",
    "    29: 'energy_freq_bands_2', 30: 'energy_freq_bands_3', 31: 'energy_freq_bands_4', \n",
    "    32: 'spect_edge_freq', 33: 'wavelet_coef_energy_1', 34: 'wavelet_coef_energy_2', \n",
    "    35: 'wavelet_coef_energy_3', 36: 'wavelet_coef_energy_4', 37: 'wavelet_coef_energy_5', \n",
    "    38: 'wavelet_coef_energy_6', 39: 'teager_kaiser_energy_1', 40: 'teager_kaiser_energy_2', \n",
    "    41: 'teager_kaiser_energy_3', 42: 'teager_kaiser_energy_4', 43: 'teager_kaiser_energy_5', \n",
    "    44: 'teager_kaiser_energy_6', 45: 'teager_kaiser_energy_7', 46: 'teager_kaiser_energy_8', \n",
    "    47: 'teager_kaiser_energy_9', 48: 'teager_kaiser_energy_10', 49: 'teager_kaiser_energy_11', \n",
    "    50: 'teager_kaiser_energy_12', 51: 'teager_kaiser_energy_13', 52: 'teager_kaiser_energy_14'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of significant cases per feature column\n",
    "feature_counts = df_significant['Feature'].value_counts().sort_index()\n",
    "\n",
    "# Replace the column indices with their corresponding feature names\n",
    "feature_counts.index = feature_counts.index.map(feature_mapping)\n",
    "\n",
    "# Plot the bar chart with feature names on the x-axis\n",
    "feature_counts.plot(kind='bar', title='Significant Cases by Feature')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Count of Significant Cases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of indeterminate cases per feature column\n",
    "feature_counts = df_indeterminate_cases['Feature'].value_counts().sort_index()\n",
    "\n",
    "# Replace the column indices with their corresponding feature names\n",
    "feature_counts.index = feature_counts.index.map(feature_mapping)\n",
    "\n",
    "# Plot the bar chart with feature names on the x-axis\n",
    "feature_counts.plot(kind='bar', title='Indeterminate Cases by Feature')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Count of Indeterminate Cases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Count the occurrences of \"no significant difference\" cases per feature column\n",
    "feature_counts = df_no_significant_diff['Feature'].value_counts().sort_index()\n",
    "\n",
    "# Replace the column indices with their corresponding feature names\n",
    "feature_counts.index = feature_counts.index.map(feature_mapping)\n",
    "\n",
    "# Plot the bar chart with feature names on the x-axis\n",
    "feature_counts.plot(kind='bar', title='No Significant Difference Cases by Feature')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Feature')\n",
    "plt.ylabel('Count of No Significant Difference Cases')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "features-eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
